{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "945b2034",
   "metadata": {},
   "source": [
    "# Код для извлечения чистого текста из html-кода с сохранением его структуры (абзацы, заголовки)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9301e6b7",
   "metadata": {},
   "source": [
    "Этот код позволяет извлекать текст из html кода с пометками соответствующих абзацев. Список абзацев можно найти в массивах headings и paragraphs. Под каждый из массивов есть условный знак, с которым хранится абзац соответствующего типа. При желании его можно разнообразить."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6bab6b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "//@This is a heading\n",
      "//&This is a paragraph.\n",
      "//&Another paragraph.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "sample_html_code = '''\n",
    "<html>\n",
    "  <body>\n",
    "    <h1>This is a heading</h1>\n",
    "    <p>This is a paragraph.</p>\n",
    "    <p>Another paragraph.</p>\n",
    "  </body>\n",
    "</html>\n",
    "'''\n",
    "\n",
    "def MarkText(html_code : str):\n",
    "\n",
    "    soup = BeautifulSoup(html_code, 'html.parser')\n",
    "\n",
    "    headings = ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']\n",
    "    paragraphs = ['p', 'pre']\n",
    "\n",
    "    HEADING_MARK = '//@'\n",
    "    PARAGRAPH_MARK = '//&'\n",
    "\n",
    "    def process_tag(tag):\n",
    "        if tag.name in headings:\n",
    "            return HEADING_MARK + tag.text.strip() + '\\n'\n",
    "        elif tag.name in paragraphs:\n",
    "            return PARAGRAPH_MARK + tag.text.strip() + '\\n'\n",
    "        else:\n",
    "            return ''\n",
    "\n",
    "\n",
    "    result = ''\n",
    "\n",
    "    for tag in soup.find_all():\n",
    "        result += process_tag(tag)\n",
    "\n",
    "    return result\n",
    "print(MarkText(sample_html_code))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83cbb524",
   "metadata": {},
   "source": [
    "# Код для сохранения иерархической связи между страницами веб-сайтов с использованием парсера Морозова"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372ca3d9",
   "metadata": {},
   "source": [
    "Здесь мы подключаемся к удалённой реляционной базе данных, создаём базу pages с полями, соответствующими логике ниже и создаём курсор для итерации по базе данных."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb21f2ae",
   "metadata": {},
   "source": [
    "CREATE TABLE Pages (\n",
    "\n",
    "    PageID INT PRIMARY KEY,\n",
    "    ParentPageID INT,\n",
    "    PageLevel INT\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a0a6a860",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlite3.Cursor at 0x17459386d40>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sqlite3\n",
    "\n",
    "conn = sqlite3.connect('history.db')\n",
    "cur = conn.cursor()\n",
    "\n",
    "cur.execute('''CREATE TABLE Pages (\n",
    "    PageID INT PRIMARY KEY,\n",
    "    ParentPageID INT,\n",
    "    PageLevel INT\n",
    ")''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe49ca9",
   "metadata": {},
   "source": [
    "Мы пока что не получили готовый парсер от ребят, поэтому вставили наш код в парсер Морозова, делавшего его раньше. Для этого мы использовали словарь вида ссылка:id_страницы. Вставляем строки с очередным сайтом в иерархическую таблицу. Также была переписана рекурсивная часть обработки ссылок Морозова на алгоритм поиск в ширину, чтобы более корректно выставлять уровни важных ссылок. Мы не смогли его протестировать, потому что таблица raex_list, из которой брались данные, видимо хранилась у Морозова локально."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1fc50ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_id = {}\n",
    "cur.execute(\"SELECT COUNT(*)\")\n",
    "first_id = cur.fetchone()[0] + 1\n",
    "def parsing_site(company, url, depth, max_depth=0):\n",
    "#     print(url)\n",
    "    cur.execute(\"INSERT INTO Pages (ParentPageID, PageLevel) VALUES (?, ?)\", (-1, 0))\n",
    "    url_id[url] = first_id\n",
    "    first_id += 1\n",
    "    internalLinks = [(url, '/', 0)] #  Создаем массив ссылок, которые обрабатываем обходом в ширину.\n",
    "    used = []\n",
    "    path_from = {}\n",
    "    while internalLinks:\n",
    "        #  new\n",
    "        now = internalLinks.pop(0)\n",
    "        url = now[0]\n",
    "        from_url = now[1]\n",
    "        depth = now[2]\n",
    "        #  Помечаем ссылку как использованную, чтоб не зациклиться.\n",
    "        used.append(url)\n",
    "        # Сохраняем ссылку откуда пришли, чтобы потом восстановить путь до корня.\n",
    "        path_from[url] = from_url\n",
    "        #  new\n",
    "\n",
    "        response = requests.get(url=url, timeout=timeout, headers=headers, verify=False)\n",
    "        soup = BeautifulSoup(response.content, 'lxml')\n",
    "\n",
    "        for script in soup([\"script\", \"style\"]):\n",
    "            script.extract()\n",
    "\n",
    "        text = soup.get_text()\n",
    "\n",
    "        lines = (line.strip() for line in text.splitlines())\n",
    "        chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "        text = '\\n'.join(chunk for chunk in chunks if chunk).replace('-\\n', '').lower()\n",
    "        text = re.sub(r'[`!@#$%^&*()_+\\-=\\[\\]{};\\':\"\\\\|,.<>\\/?~©«»—]', r'', text)\n",
    "        text = ''.join([i for i in text if not i.isdigit()])\n",
    "\n",
    "        text = list(np.concatenate([sent_tokenize(i.strip()) for i in text.split('\\n')]).flat)\n",
    "\n",
    "        text_lemmatized = []\n",
    "        for line in [t.split() for t in text]:\n",
    "            line_lemmatized = ' '.join([morph.normal_forms(l)[0] for l in line if l not in stopwords_ru])\n",
    "            text_lemmatized.append(line_lemmatized)\n",
    "\n",
    "        spans = list(filter(None, [span.string for span in soup.find_all('span')]))\n",
    "\n",
    "        if len(spans):\n",
    "            spans = '\\n'.join(span for span in spans if span).replace('-\\n', '').lower()\n",
    "            spans = re.sub(r'[`!@#$%^&*()_+\\-=\\[\\]{};\\':\"\\\\|,.<>\\/?~©«»—]', r'', spans)\n",
    "            spans = ''.join([i for i in spans if not i.isdigit()])\n",
    "\n",
    "            spans = list(np.concatenate([sent_tokenize(i.strip()) for i in spans.split('\\n')]).flat)\n",
    "        else:\n",
    "            spans = []\n",
    "\n",
    "        spans_lemmatized = []\n",
    "        for line in [s.split() for s in spans]:\n",
    "            line_lemmatized = ' '.join([morph.normal_forms(l)[0] for l in line if l not in stopwords_ru])\n",
    "            spans_lemmatized.append(line_lemmatized)\n",
    "\n",
    "        df1 = pd.DataFrame(text_lemmatized, columns=['text'])\n",
    "        df2 = pd.DataFrame(spans_lemmatized, columns=['text'])\n",
    "        data = pd.concat([df1, df2]).drop_duplicates().reset_index(drop=True)\n",
    "        data['company'] = company['Название']\n",
    "        data['rating'] = company['№']\n",
    "        data['url'] = url\n",
    "        data = data[['rating', 'company', 'url', 'text']]\n",
    "\n",
    "        # Считываем все ссылки на сайте и добавляем их в конец нашей очереди обхода в ширину.\n",
    "        # Проверяем так же глубину обхода - слишком далеко нам уходить не надо.\n",
    "        if depth < max_depth:\n",
    "            NewLinks = [\n",
    "                a.get('href') for a in soup.find_all('a')\n",
    "                if a.get('href') and a.get('href').startswith('/')\n",
    "            ]\n",
    "            NewLinks = [link[1:] for link in NewLinks]\n",
    "            NewLinks = [*set(NewLinks)]\n",
    "            NewLinks = list(filter(None, NewLinks))\n",
    "\n",
    "            for link in NewLinks:\n",
    "                    if \".pdf\" not in link and \".PDF\" not in link:\n",
    "                        new_url = os.path.join(url[:url.rfind('www')], urlparse(url).netloc, link)\n",
    "                        #Тут как раз добавляем URL в конец очереди.\n",
    "                        if new_url not in used:\n",
    "                            internalLinks.append((new_url, url, depth + 1))\n",
    "                            cur.execute(\"INSERT INTO Pages (ParentPageID, PageLevel) VALUES (?, ?)\", (url_id[url], depth + 1))\n",
    "                            url_id[new_url] = first_id\n",
    "                            first_id += 1\n",
    "\n",
    "\n",
    "        #  Здесь вставляем наш PAGE_ID\n",
    "        #  Путь до корневой папки и будет иерархической определяющей нашей страницы.\n",
    "        #  Восстанавливается он следующим образом :\n",
    "        path_now = []\n",
    "        while url != '/' :\n",
    "            path_now.append(url)\n",
    "            url = path_from[url]\n",
    "        path_now.reverse()\n",
    "\n",
    "        # В path_now хранится путь до страницы из корня!\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc4ac0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
